\chapter{Design \& Implementation}\label{sec:impl}

\section{Bitwidth analysis}
The analysis is a data flow analysis. The analysis attaches an $(int,boolean)$ tuple to every meaningful node. A node is considered meaningful if the node has an integer-mode. We will reference the first value as \emph{stable bits} and the second bit as \emph{is positive}. \newline
What stable here means can be observed when looking at \ref{fig:numbers}.
For the first line of the table we have five stable digits. The second line has 7 stable digits.\newline
Since this is a data flow analysis, which is built on an iterative approach. We might want to address the current tuple of a node $x$, while we have a new one calculated. Therefore we call $\hat{x}$ the currently associated value, and $x$ the newly calculated one.

\paragraph{Bit representation}
The stable bits indicate how many bits are stable, and therefore not used.
The second value of the tuple indicates if the value will ever reach negative numbers or not. Thus it indicates at least one stable bit at the highest position. However, the second value is only meaningful for modes that allow signs.

\paragraph{Range representation}
There is also a second way of interpreting the two values. The stable bits can define a minimum and maximum range. The maximum number is reached if the stable bits are all zero, and the rest one. If the mode is signed and the node is not positive, then the minimum number is reached by assuming all stable bits are one, and the rest zero. Otherwise the minimum is 0. We can define the following min max definitions for the ranges:

$
max_{bitwidth}(x)=
\left\{
\begin{array}{l}2^{stable\_digits-1}-1\\2^{stable\_digits}-1\end{array}
\begin{array}{l} {mode.signed} \\ {Otherwise} \end{array}
\right.
$

$
min_{bitwidth}(x)=
\left\{
\begin{array}{l}2^{stable\_digits-1}\\0\end{array}
\begin{array}{l} {mode.signed \wedge is\_positive} \\ {Otherwise} \end{array}
\right.
$

\input{fig/lattice.tex}

\paragraph{Analysis}
\label{analysis_explain}
The analysis works as a fixed point iteration. For it we use  \autoref{fig:lattice} as lattice. The values of the lattice represent the tuples from the analysis.

As a first step, we iterate over every single node and initialize the node with $\top$ and mark it as \textit{dirty}. If the node is constant, we calculate its bitwidth. Nodes with the opcodes \textit{Const}, \textit{Size} and \textit{Address} are considered constant.

The second step consists of recalculating every \textit{dirty} node in the graph. 
if the stable bits of $x$ are smaller than those from $\hat{x}$, 
then the new value is memorized as $\hat{x}$ of the node. Also every successor of the node is marked as dirty. The used rules for recalculating the nodes are described in \autoref{table_of_rules}.

\subsection{Value prediction}
In addition to the normal analysis results, the fixed point iteration can insert additional confirm nodes. Those confirm nodes help making the analysis more accurate.
First of all we need a few definitions for easier understanding:

\input{fig/upper_bound_cmp.tex}

\subparagraph{Definition: Upper bounds}
A compare node defines an upper bound if the relation is $<$ and the second operand is constant.\newline
The definition also applies for compare nodes that can be transformed by swapping the two operands and the relation accordingly.

\subparagraph{Definition: Predecessor in a certain block}
In the detection described later, we often need to find a predecessor that is placed in a certain block. Therefore we define:
\begin{center}
$\kappa(a, b) := \{X| X \leftarrow a \wedge X.block = b \}$ 
\end{center}

It will return every node that is located in \textit{b} and is a predecessor of \textit{a}.

\subparagraph{Definition: Constant dependencies}
\input{fig/def_xi.tex}
While looking at C code we often see that addition and multiplication nodes are used for calculating array addresses or addresses for structure access. Therefore, one operand of the arithmetical operations is often constant. An example for this can be found in \autoref{fig:def:xi}.
We define $\xi$ to explore the whole tree of nodes with one constant and one none-constant operands, and return us every node that was not constant.
\begin{center}
$\xi(a) := 
\left\{
	\begin{array}{l}
		a \cup \xi(c)\\ 
		\emptyset
	\end{array}
	\begin{array}{l}
		, \text{If there is only one not constant dependency \textit{c}} \\ 
		, \text{otherwise}.
	\end{array}
\right.$
\end{center}

If \textit{a} has only one none-constant operand c, then $\xi$ returns the element $a$ and $\xi(c)$. Otherwise it returns an empty set. In \autoref{fig:def:xi} the highlighted nodes are part of $\xi(Y)$.

\paragraph{Upper bounds for block execution}
The values that are calculated in a node are (even if the fixed point iteration is not stable yet) within the ranges for the later stable result. The iteration starts at $\top$ and moves into the direction of $\bot$. 
%This means for a node \textit{n} our range of possible values starts at something like $[0,0]$, moving towards $[max _{bitwidth}(n), min _{bitwidth}(n)]$ with each iteration.  %CORRECTION LINE
We now evaluate an upper bound compare node, every time the first operand changes. In the beginning we can say that that with those possible values, each time the true-block will be executed. However, if $max_{bitwidth}(n)$ grows enough to get bigger than the second operand or the compare node, then we can say, that we have found a upper bound for the execution of the true-block. Which is $max _{bitwidth}(n)$ in the current state. Thus we can insert a confirm node between every node $e \in \kappa(i, j)$ and $i$, where $i$ is the first operand and j is the true block.

\paragraph{Extended confirm insertion}
The confirm nodes that we have inserted in the paragraph before can also be transported backwards.
With $\xi(i)$ we can get a set of nodes, where the current state in the analysis is only depending on one node. Thus we can say that the state of every node from $\xi(i)$ will not change as long as the topmost upper element in the tree structure does not change. Which means that we can insert a confirm node between $(e,g)$ for every $e \in \xi(i)$ and $g \in \kappa(e, j)$

\section{Stable Conversion nodes}
In libfirm a conversion node can be used to convert a value from one mode to another. This type of node has one operand. A conversion like this can have one of two effects. The value stays the same, or the value changes, due to the inability of displaying the value in a different mode. We call the first case \textit{stable conversion node}. A example for an unstable conversion node may look like ($unsigned$)(($int$)-10). A stable conversion node may look like ($unsigned$)(($int$)10).

\paragraph{Finding stable conversion nodes}
Such stable conversion nodes can be found using the bitwidth analysis. We compare the range of the operand with the range of the conversion node itself. If the ranges are the same, then we know that the conversion is stable.

\paragraph{Removing conversion nodes}
\input{fig/conv_opt.tex}
In case we found a stable conversion node, we can say that this node only exists for syntax rules, there is no semantical value in it. Removing those nodes also has the advantage of helping other analyses. 
The confirm insertion algorithm of $\libFIRM$  searches for assertions that can be made based on looking at compare nodes. This works quite well. However, the example in \autoref{fig:example:conversion_opt} does not work.
The insertion code could only insert a Confirm between the Compare and the conversion. However, the Confirm node would need to be an operand of the conversion node, in order to help the rest of the code, since other nodes will likely depend on the operand of the conversion node and not on the conversion node itself. 
After removing the conversion node, the analysis can find an assertion based on the compare node. This could help the branch prediction and dead code elimination.

However, for really removing the conversion nodes, we need to find situations where we can eliminate the conversion node. We have already seen an example with a compare node. Additionally we can do the same with an arithmetical operation.

\paragraph{Compare-Conversion optimization}
The rule in $\libFIRM$ is that the two operands of a compare node have to have the same mode. This means, when we remove the conversion node, we also need to adjust the mode of the second operand. This is not easily possible for a none-Constant node.
Thus we confine for now that the the operand needs to be a Conversion node, and the second one a Constant node. With this we can simple adjust its mode, and remove the conversion.

\paragraph{Arithmetical-Conversion optimization}
\input{fig/arith_conv.tex}
The situation of arithmetical nodes, with one operation being constant, one operation  being a conversion, can also be optimized. In this case we move it through the arithmetical operation, and place it afterwards. In the same time we change the mode of the constant and the arithmetical node to the earlier node. We do this in order to hope that we can remove it with a compare conversion optimization after that.
Such a construct might look like \autoref{fig:example:arithmetical_opt} . After we adjusted the mode of the constant we can move the conversion from the operand of the arithmetical operation to the end.

\section{VHDL generation}

There is a $\libFIRM$ tool called firm2vhdl. The tool takes the output from the \textit{cparser} compiler, and outputs the VHDL. Every node in the firm graph gets transformed into a VHDL statement. This is done by transforming the operation of the nodes into VHDL code. Each result of a node operation is assigned to a new variable, which then can be used again later by the next operation. 
Each of those those variables are represented in hardware, and thus have an certain amount of bits. The maximum amount of bits that is needed, can be calculated by using the bitwidth analysis.

\paragraph{bitwidth in firm2vhdl}
The amount of bits per variable were previously just the amount of bits the mode of a node needs. In VHDL this wastes a lot of space on the FPGA chip later on. Minimizing the amount of bits used per variable here can be important, since most of the variables used in C do not use the complete bitwidth.\newline
The bitwidth information gathered from the analysis can help here, as it defines how many bits of a node are used, and how many are not. Thus we can add code to the transformation, for taking the bitwidth, instead of the number of bits in a mode.

%FIXME example

\section{How about section}
This section is about decisions that where made, and why they are made.

\subsection{Value range vs. bitwidth}

%Where is the actaul difference when looking at the rules.
The analysis explained in \ref{analysis_explain} is using rules on the nodes that are approximately calculating the bitwidth that is used by the CFG. The same algorithm could use rules that are changing the range, and not the bitwidth. The later would be called value range propagation. The result of both analysis are similar, while the VRP is slightly more accurate in some situations than the Bitwidth analysis. Another justification therefore can be found in ???.

However, the paper misses one essential fact. In case of a loop (???) where no assertions can be made, both analysis share the same result. Which is that the nodes will use their whole bitwidth / range. However, the difference between the two analysis here is, that the VRP will need $2^{msb}$ iterations, while the bitwidth analysis only takes $msb$ iterations, where $msb$ expresses the number of the maximum bit. Thus the bitwidth analysis produces the same result, while it's a lot faster.

Another interesting situation is when a Confirm node can be inserted in a loop. If this is the case, then the bitwidth analysis will return $2^{TOP log2(bound) TOP}$ while the VRP would return $log2(bound)$ as the biggest number required by the code. What this shows is, that the VRP here is only more accurate, because the data structure of the bitwidth info operates on bits, while the VRP operates on normal numbers. The runtime here is the same.

\label{VRP_king}
The only configuration where the VRP returns constantly a more accurate result, is when arithmetical operations are not looping, but are rather used as some sort of tree structure, like it is illustrated in ???. Here the VRP is more accurate, the runtime is the same.

\subsection{Widening \& Narrowing}

Widening and Narrowing is a technique that tries to achieve the same or slightly worse results, by maintaining a better runtime. Here are a few things that could be done in this analysis to achive this. However, the time was short and thus the following is only thought about, but not yet implemented.

\paragraph{Widening not terminating loops}
There is the theoretical question of when we are able to predict that a loop will terminate before the whole bitwidth is used or not. The question for this analysis is quite simple, if there is a Compare node in the loop, and it is defining a upper bound, then we might find a upper bound when we can insert the confirm node there as described in ???. If there is no Confirm node, and no Compare node, then there is no chance for the loop to terminate. And thus we might want to wide our stable bits to 0.

\paragraph{Narrowing for arithmetical chains}
As explained in \ref{VRP_king}, the VRP returns a more accurate result for arithmetical chains. A arithmetical chain can be defined as a set of arithmetical nodes, where each node has a successor which is a arithmetical node.
%explain the problem
The problem can be fixed, by calculating the result for a arithmetical node as the bitwidth information, as well as the exact range. The successor of the node can then use the exact range instead of the bitwidth info for calculating its results. Thus we end up in the same result as the VRP.
